{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67504d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "\n",
    "#notebook_login()\n",
    "# Set the Hugging Face token as an environment variable\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b315e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Model configs\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "load_adapter = False\n",
    "mlflow_uri = \"file:../mlruns\"\n",
    "# mlflow_experiment_name = \"Approach_1_Multiple_Schemas\"\n",
    "mlflow_experiment_name = \"Approach_2_Property_Style\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eaa1d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run ID: 3fb5ee5e6f584fda9aecfeebf320b5fe\n",
      "Artifact URI: c:\\Users\\Admin\\Documents\\Projects\\Repositories\\synthetic-data-generator/mlruns/232166607607031798/7aceb521194d4286be3e485efc1272f8/artifacts/adapters\\epoch-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:40: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\e'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_21356\\224126963.py:40: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  adapter_dir = \"c:\\\\Users\\\\Admin\\\\Documents\\\\Projects\\\\Repositories\\\\synthetic-data-generator/mlruns/232166607607031798/7aceb521194d4286be3e485efc1272f8/artifacts/adapters\\epoch-4\"\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "def get_last_run_id():\n",
    "    runs = mlflow.search_runs(order_by=[\"start_time desc\"], max_results=1)\n",
    "    if not runs.empty:\n",
    "        return runs.iloc[0].run_id\n",
    "    else:\n",
    "        logger.warning(\"No runs found in MLflow.\")\n",
    "        return None\n",
    "\n",
    "def get_run_artifact_uri(last_run_id):\n",
    "    if last_run_id:\n",
    "        run_info = mlflow.get_run(last_run_id)\n",
    "        artifact_path = run_info.info.artifact_uri\n",
    "        if artifact_path.startswith(\"file://\"):\n",
    "            artifact_path = artifact_path.replace(\"file://\", \"\")\n",
    "        elif artifact_path.startswith(\"file:\"):\n",
    "            artifact_path = artifact_path.replace(\"file:\", \"\")\n",
    "        if \"/training/..\" in artifact_path:\n",
    "            artifact_path = artifact_path.replace(\"/training/..\", \"\")\n",
    "        artifact_path = artifact_path+ \"/adapters\"\n",
    "        folders = [f for f in os.listdir(artifact_path) if os.path.isdir(os.path.join(artifact_path, f))]\n",
    "        logger.info(f\"Models available: {', '.join(folders)}\")\n",
    "        if folders:\n",
    "            last_updated_folder = max(folders, key=lambda f: os.path.getmtime(os.path.join(artifact_path, f)))\n",
    "            logger.info(f\"Last updated model: {last_updated_folder}\")\n",
    "            return os.path.join(artifact_path, last_updated_folder)\n",
    "        else:\n",
    "            logger.warning(\"No folders found in the artifact URI.\")\n",
    "        return None\n",
    "    else:\n",
    "        logger.warning(\"No runs found in MLflow.\")\n",
    "        return None\n",
    "    \n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n",
    "\n",
    "run_id = get_last_run_id()\n",
    "adapter_dir = get_run_artifact_uri(run_id)\n",
    "adapter_dir = \"c:\\\\Users\\\\Admin\\\\Documents\\\\Projects\\\\Repositories\\\\synthetic-data-generator/mlruns/232166607607031798/7aceb521194d4286be3e485efc1272f8/artifacts/adapters\\epoch-4\"\n",
    "\n",
    "print(f\"Last run ID: {run_id}\")\n",
    "print(f\"Artifact URI: {adapter_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb99e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a554aa6a17e404eb54021512c551355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                  quantization_config=quantization_config , \n",
    "                                                  device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load QLoRA adapter on top\n",
    "if load_adapter:\n",
    "    logger.info(f\"Loading adapter from {adapter_dir}\")\n",
    "    model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_dir)\n",
    "\n",
    "logger.info(f\"Model loaded with {model_name} and adapter from {adapter_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c37b6b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  You are a Json test data generator generating valid json data based on a json schema. Given the below schema,\n",
      "### Schema: \n",
      "{\n",
      "    \"type\": \"object\",\n",
      "    \"properties\": {\n",
      "        \"name\": {\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        \"age\": {\n",
      "            \"type\": \"integer\",\n",
      "            \"minimum\": 0\n",
      "        },\n",
      "        \"email\": {\n",
      "            \"type\": \"string\",\n",
      "            \"format\": \"email\"\n",
      "        },\n",
      "        \"claimstatus\":{\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"status\": {\n",
      "                    \"type\": \"string\",\n",
      "                    \"enum\": [\"pending\", \"approved\", \"rejected\"]\n",
      "                },\n",
      "                \"reason\": {\n",
      "                    \"type\": \"string\"\n",
      "                }\n",
      "            },\n",
      "            \"required\": [\"status\"]\n",
      "        }\n",
      "    },\n",
      "    \"required\": [\"name\", \"age\", \"email\"]\n",
      "} \n",
      "Generate a sample json for the above schema.\n",
      "  {\"name\": \"James\", \"age\": 23, \"email\": \"james.rodriguez@gmail.com\", \"claimstatus\": {\"status\": \"pending\", \"reason\": \"Additional Information Needed\"}}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\" <s>[INST]You are a Json test data generator generating valid json data based on a json schema. Given the below schema,\n",
    "### Schema: \n",
    "{\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"age\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"minimum\": 0\n",
    "        },\n",
    "        \"email\": {\n",
    "            \"type\": \"string\",\n",
    "            \"format\": \"email\"\n",
    "        },\n",
    "        \"claimstatus\":{\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"status\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"pending\", \"approved\", \"rejected\"]\n",
    "                },\n",
    "                \"reason\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"status\"]\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\", \"email\"]\n",
    "} \n",
    "Generate a sample json for the above schema.\n",
    "[/INST] \"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Output: \", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthetic-data-generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
