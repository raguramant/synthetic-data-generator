{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b5f461",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8b8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers import BitsAndBytesConfig\n",
    "#Model configs\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "training_data = \"../data/schema_style_dataset_v1.jsonl\"\n",
    "\n",
    "mlflow_uri = \"file:../mlruns\"\n",
    "mlflow_experiment_name = \"Approach_1_Multiple_Schemas\"\n",
    "# mlflow_experiment_name = \"Approach_2_Property_Style\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78af39bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 12 22:56:05 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.40                 Driver Version: 576.40         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5070 Ti   WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   43C    P8             19W /  300W |    8614MiB /  16303MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           19124      C   ...tic-data-generator\\python.exe      N/A      |\n",
      "|    0   N/A  N/A           23588      C   ...nstaller_files\\env\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdcf5407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - NumExpr defaulting to 16 threads.\n",
      "INFO - PyTorch version 2.8.0+cu129 available.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "\n",
    "# load the dataset and split it into train, validation and test sets\n",
    "dataset = load_dataset(\"json\", data_files=training_data, split='train')\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "train_temp_split = shuffled_dataset.train_test_split(test_size=0.3) #30% for validation and test\n",
    "temp_dataset = train_temp_split['test']\n",
    "validation_test_split = temp_dataset.train_test_split(test_size=1/3)# 10% for validation and 20% for test\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_temp_split['train'],\n",
    "    'validation': validation_test_split['train'],\n",
    "    'test': validation_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "940c9b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run ID: 37a27a5be12943d5b77bfcb0044256af\n",
      "Artifact URI: c:\\Users\\Admin\\Documents\\Projects\\Repositories\\synthetic-data-generator/mlruns/232166607607031798/37a27a5be12943d5b77bfcb0044256af/artifacts/adapters/epoch-3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "def get_last_run_id():\n",
    "    runs = mlflow.search_runs(order_by=[\"start_time desc\"], max_results=1)\n",
    "    if not runs.empty:\n",
    "        return runs.iloc[0].run_id\n",
    "    else:\n",
    "        logger.warning(\"No runs found in MLflow.\")\n",
    "        return None\n",
    "\n",
    "def get_run_artifact_uri(last_run_id):\n",
    "    if last_run_id:\n",
    "        run_info = mlflow.get_run(last_run_id)\n",
    "        artifact_path = run_info.info.artifact_uri\n",
    "        if artifact_path.startswith(\"file://\"):\n",
    "            artifact_path = artifact_path.replace(\"file://\", \"\")\n",
    "        elif artifact_path.startswith(\"file:\"):\n",
    "            artifact_path = artifact_path.replace(\"file:\", \"\")\n",
    "        if \"/training/..\" in artifact_path:\n",
    "            artifact_path = artifact_path.replace(\"/training/..\", \"\")\n",
    "        artifact_path = artifact_path+ \"/adapters\"\n",
    "        folders = [f for f in os.listdir(artifact_path) if os.path.isdir(os.path.join(artifact_path, f))]\n",
    "        logger.info(f\"Models available: {', '.join(folders)}\")\n",
    "        if folders:\n",
    "            last_updated_folder = max(folders, key=lambda f: os.path.getmtime(os.path.join(artifact_path, f)))\n",
    "            logger.info(f\"Last updated model: {last_updated_folder}\")\n",
    "            return os.path.join(artifact_path, last_updated_folder)\n",
    "        else:\n",
    "            logger.warning(\"No folders found in the artifact URI.\")\n",
    "        return None\n",
    "    else:\n",
    "        logger.warning(\"No runs found in MLflow.\")\n",
    "        return None\n",
    "    \n",
    "mlflow.end_run()\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n",
    "\n",
    "run_id = get_last_run_id()\n",
    "mlflow.start_run(run_id=run_id)\n",
    "# adapter_dir = get_run_artifact_uri(run_id)\n",
    "adapter_dir = \"c:\\\\Users\\\\Admin\\\\Documents\\\\Projects\\\\Repositories\\\\synthetic-data-generator/mlruns/232166607607031798/37a27a5be12943d5b77bfcb0044256af/artifacts/adapters/epoch-3\"\n",
    "\n",
    "print(f\"Last run ID: {run_id}\")\n",
    "print(f\"Artifact URI: {adapter_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "512bd59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106ee6334d6942d1a5a71550365ca231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                    quantization_config=quantization_config , \n",
    "                                                    device_map=\"auto\")\n",
    "    \n",
    "model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228faf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from jsonschema import validate, ValidationError\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "# function to extact JSON from a string\n",
    "def find_and_parse_json(text: str) -> dict:\n",
    "\n",
    "    try:\n",
    "        # Find the first opening curly brace\n",
    "        first_brace_index = text.find('{')\n",
    "        if first_brace_index == -1:\n",
    "            return None # No JSON object found\n",
    "\n",
    "        # Start searching from the first brace\n",
    "        brace_level = 1\n",
    "        for i, char in enumerate(text[first_brace_index + 1:]):\n",
    "            if char == '{':\n",
    "                brace_level += 1\n",
    "            elif char == '}':\n",
    "                brace_level -= 1\n",
    "            \n",
    "            if brace_level == 0:\n",
    "                # We found the matching closing brace\n",
    "                last_brace_index = first_brace_index + i + 1\n",
    "                json_string = text[first_brace_index : last_brace_index + 1]\n",
    "                \n",
    "                # Now, use the built-in json library to parse it\n",
    "                return json.loads(json_string)\n",
    "        \n",
    "        return None # No complete JSON object found\n",
    "    except (json.JSONDecodeError, IndexError):\n",
    "        # Handle cases where the substring is not valid JSON or string is malformed\n",
    "        return None\n",
    "\n",
    "\n",
    "# Validate a JSON string against a schema\n",
    "def is_valid_json_schema(json_generated, schema):\n",
    "    try:\n",
    "        validate(instance=json_generated, schema=schema)\n",
    "        return True\n",
    "    except (json.JSONDecodeError, ValidationError):\n",
    "        return False\n",
    "    \n",
    "# domainn specific checks\n",
    "def domain_specific_checks(json_ref, json_gen, domain_field, pattern):\n",
    "    score = 0\n",
    "    if domain_field in json_ref.keys():\n",
    "        # Check if NM1 segment is present in both JSONs\n",
    "        logger.debug(f\"{json_ref[domain_field]}  {json_gen.get(domain_field)}\")\n",
    "        if domain_field in json_gen.keys():\n",
    "            #NM1*PR*2*Palmer*Darlene*M*Mr.*DVM*PI*SqLFzolMNdaVXE~\n",
    "            # pattern = r'^NM1(?:\\*[^*]*){9}~$'\n",
    "            value = json_gen[domain_field]\n",
    "            if not bool(re.match(pattern, value)):\n",
    "                score += 2.5\n",
    "                logger.debug(f\"{domain_field} {value} does not match the expected pattern, reducing score by 2.5\")\n",
    "            # check if all values in the json_gen are present in the value except for the nm1_segment\n",
    "            for key in json_gen.keys():\n",
    "                if key != domain_field and json_gen[key] not in value:\n",
    "                    score += 2.5\n",
    "                    logger.debug(f\"Value {json_gen[key]} not found in {domain_field} {value}, reducing score by 2.5\")\n",
    "                    break\n",
    "        else:\n",
    "            score += 5\n",
    "            logger.debug(f\"{domain_field} not found in json_gen, reducing score by 5\")\n",
    "\n",
    "    return score\n",
    "    \n",
    "\n",
    "# Compare two JSON strings at field level and match field values\n",
    "def compare_json_field_values(json_ref, json_gen):\n",
    "   \n",
    "    if not isinstance(json_ref, dict) or not isinstance(json_gen, dict):\n",
    "        return 0.0\n",
    "    \n",
    "    score = 10\n",
    "    # json_gen has all fields of json_ref, if not reduce the score proportional to no of fields missing\n",
    "    logger.debug(f\"Starting score: {score}\")\n",
    "    if set(json_gen.keys()).issubset(set(json_ref.keys())):\n",
    "        for key in json_ref.keys():\n",
    "            if json_gen.get(key) is None:\n",
    "                reduce_score = 3 / len(json_ref.keys())\n",
    "                logger.debug(f\"Key {key} is missing in json_gen, reducing score by {reduce_score}\")\n",
    "                score -= reduce_score\n",
    "    else:\n",
    "        reduce_score = (len(json_ref.keys()) - len(json_gen.keys())) * (3 / len(json_ref.keys()))\n",
    "        logger.debug(f\"Keys in json_ref not present in json_gen, reducing score by {reduce_score}\")\n",
    "        if reduce_score > 0:\n",
    "            score -= reduce_score\n",
    "\n",
    "    # domainn specific checks\n",
    "    score -= domain_specific_checks(json_ref, json_gen, \"nm1_segment\", r'^NM1(?:\\*[^*]*){9}~$')\n",
    "    score -= domain_specific_checks(json_ref, json_gen, \"dtp_segment\", r'^DTP(?:\\*[^*]*){3}~$')\n",
    "    score -= domain_specific_checks(json_ref, json_gen, \"ref_segment\", r'^REF(?:\\*[^*]*){2}~$')\n",
    "    score -= domain_specific_checks(json_ref, json_gen, \"isa_segment\", r'^ISA(?:\\*[^*]*){16}~$')\n",
    "\n",
    "    # If the score is less than 0, set it to 0\n",
    "    if score < 0.0:\n",
    "        score = 0.0\n",
    "\n",
    "    logger.debug(f\"Final score: {score}\")\n",
    "\n",
    "    return score\n",
    "\n",
    "def get_result(valid_json, valid_json_schema, comparison_results):\n",
    "    return json.dumps({\n",
    "        \"valid_json\": np.mean(valid_json) * 100,\n",
    "        \"valid_json_schema\": np.mean(valid_json_schema) * 100,\n",
    "        \"domain_field_match\": np.mean(comparison_results) * 10\n",
    "    })\n",
    "\n",
    "\n",
    "def test_model(model, tokenizer, test_dataset):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    valid_json = []\n",
    "    valid_json_schema = []\n",
    "    comparison_results = []\n",
    "\n",
    "    stop_words = [\"[INST]\"]\n",
    "    stop_word_ids = [tokenizer(stop_word, add_special_tokens=False).input_ids[0] for stop_word in stop_words]\n",
    "    stop_word_ids.append(tokenizer.eos_token_id)  # Add EOS token ID to the stop words\n",
    "    logger.info(f\"Tokens to stop on: {stop_words}\")\n",
    "    logger.info(f\"Corresponding Token IDs: {stop_word_ids}\")\n",
    "        \n",
    "    for i, row in enumerate(test_dataset):\n",
    "\n",
    "        logger.info(f\"Processing row {i+1}/{len(test_dataset)}\")\n",
    "\n",
    "        prompt = f\"<s>[INST] {row['messages'][0]['content']} [/INST]\"\n",
    "        # print(f\"Prompt: \\n {prompt} \\n ---------------------------------\")\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1000,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,                \n",
    "                top_p=0.9,\n",
    "                eos_token_id=stop_word_ids\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "        logger.info(f\"Generated text: \\n {generated_text}\")\n",
    "        \n",
    "        if(\"[/INST]\" in generated_text):\n",
    "            generated_json = find_and_parse_json(generated_text.split(\"[/INST]\")[-1].strip())\n",
    "            gold_json = find_and_parse_json(row['messages'][1]['content'])\n",
    "            schema = find_and_parse_json(generated_text.split(\"[/INST]\")[-2].strip())\n",
    "            # print(f\"Generated JSON: {generated_json} \\n Goled JSON: {gold_json} \\n Schema: {schema}\")\n",
    "        \n",
    "        # is Json valid?\n",
    "        if generated_json is not None:\n",
    "            valid_json.append(True)\n",
    "        else:\n",
    "            valid_json.append(False)\n",
    "\n",
    "        # is JSON valid against schema?\n",
    "        if schema is not None:\n",
    "            if generated_json is not None and is_valid_json_schema(generated_json, schema):\n",
    "                valid_json_schema.append(True)\n",
    "            else:\n",
    "                valid_json_schema.append(False)\n",
    "        \n",
    "        # compare JSON field values\n",
    "        if gold_json is not None:\n",
    "            if generated_json is not None:\n",
    "                comparison_score = compare_json_field_values(gold_json, generated_json)\n",
    "            else:\n",
    "                comparison_score = 0.0\n",
    "            comparison_results.append(comparison_score)\n",
    "\n",
    "        logger.info(get_result(valid_json, valid_json_schema, comparison_results))\n",
    "        logger.info(\"------------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "    return get_result(valid_json, valid_json_schema, comparison_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2004234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Tokens to stop on: ['[INST]']\n",
      "INFO - Corresponding Token IDs: [3, 2]\n",
      "INFO - Processing row 1/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Give a sample data for the property schema with the value: 417/04.  \n",
      " {\n",
      "  \"blood_pressure\": {\n",
      "    \"type\": \"string\",\n",
      "    \"pattern\": \"^(\\\\d{1,3})/(\\\\d{1,3})$\",\n",
      "    \"description\": \"Blood pressure of the individual (systolic/diastolic)\"\n",
      "  }\n",
      "} [/INST] {\"blood_pressure\": \"417/04\"}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 2/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Create a sample data for the property schema with the value:   172200319.  \n",
      " {\n",
      "  \"phone\": {\n",
      "    \"type\": \"string\",\n",
      "    \"pattern\": \"^\\\\+?[0-9\\\\- ]{7,15}$\",\n",
      "    \"description\": \"Phone number of the individual\"\n",
      "  }\n",
      "} [/INST] {\"phone\": \"+172200319\"}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 3/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Create a organization property with the value: tend based on the schema provided.  \n",
      " {\n",
      "  \"organization\": {\n",
      "    \"type\": \"string\",\n",
      "    \"minLength\": 1,\n",
      "    \"maxLength\": 50,\n",
      "    \"description\": \"Name of the organization\"\n",
      "  }\n",
      "} [/INST] {\"organization\": \"tend\"}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 4/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Create a benefits property with the value: ['lead', 'action'] based on the schema provided.  \n",
      " {\n",
      "  \"benefits\": {\n",
      "    \"type\": \"array\",\n",
      "    \"items\": {\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"description\": \"List of benefits covered by the insurance\"\n",
      "  }\n",
      "} [/INST] {\"benefits\": [\"lead\", \"action\"]}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 5/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Generate a doctor_name property with the value: Lauren Diaz for the given schema. \n",
      " {\n",
      "  \"doctor_name\": {\n",
      "    \"type\": \"string\",\n",
      "    \"description\": \"Name of the individual's doctor\"\n",
      "  }\n",
      "} [/INST] {\"doctor_name\": \"Lauren Diaz\"}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 6/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Create a date_of_birth property with the value: 1983-12-09 based on the schema provided.  \n",
      " {\n",
      "  \"date_of_birth\": {\n",
      "    \"type\": \"string\",\n",
      "    \"format\": \"date\",\n",
      "    \"description\": \"Date of birth of the individual\"\n",
      "  }\n",
      "} [/INST] {\"date_of_birth\": \"1983-12-09\"}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 7/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Generate a sample data for the given property schema.  \n",
      " {\n",
      "  \"medications\": {\n",
      "    \"type\": \"array\",\n",
      "    \"items\": {\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"description\": \"Medications of the individual\"\n",
      "  }\n",
      "} [/INST] {\"medications\": [\"metformin\", \"glipizide\", \"amlodipine\", \"aspirin\", \"lisinopril\", \"atorvastatin\"]}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 8/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Create a sample data for the property schema with the value: 29.25.  \n",
      " {\n",
      "  \"height_cm\": {\n",
      "    \"type\": \"number\",\n",
      "    \"minimum\": 0,\n",
      "    \"maximum\": 180,\n",
      "    \"description\": \"Height of the individual in centimeters\"\n",
      "  }\n",
      "} [/INST] {\"height_cm\": 29.25}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 9/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Generate a medical_conditions property with the value: ['job', 'about', 'know', 'teach', 'skill'] for the given schema. \n",
      " {\n",
      "  \"medical_conditions\": {\n",
      "    \"type\": \"array\",\n",
      "    \"items\": {\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"description\": \"Medical conditions of the individual\"\n",
      "  }\n",
      "} [/INST] {\"medical_conditions\": [\"job\", \"about\", \"know\", \"teach\", \"skill\"]}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 10/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Generate a sample data for the given property schema.  \n",
      " {\n",
      "  \"organization\": {\n",
      "    \"type\": \"string\",\n",
      "    \"minLength\": 1,\n",
      "    \"maxLength\": 50,\n",
      "    \"description\": \"Name of the organization\"\n",
      "  }\n",
      "} [/INST] {\"organization\": \"Samsung Electronics\"}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 11/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Generate a service_type_code property based on the schema.  \n",
      " {\n",
      "  \"service_type_code\": {\n",
      "    \"type\": \"string\",\n",
      "    \"minLength\": 1,\n",
      "    \"maxLength\": 50,\n",
      "    \"description\": \"Code representing the type of service\"\n",
      "  }\n",
      "} [/INST] service_type_code: \"sustainability program\"\n",
      "\n",
      "{\n",
      "  \"service_type_code\": \"sustainability program\"\n",
      "}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 12/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Create a patient_name property with the value: Jeffrey Moore based on the schema provided.  \n",
      " {\n",
      "  \"patient_name\": {\n",
      "    \"type\": \"string\",\n",
      "    \"minLength\": 1,\n",
      "    \"maxLength\": 50,\n",
      "    \"description\": \"Name of the patient\"\n",
      "  }\n",
      "} [/INST] {\"patient_name\": \"Jeffrey Moore\"}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 13/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Create a name property with the value: Douglas Bishop based on the schema provided.  \n",
      " {\n",
      "  \"name\": {\n",
      "    \"type\": \"string\",\n",
      "    \"minLength\": 1,\n",
      "    \"maxLength\": 50,\n",
      "    \"description\": \"Full name of the individual\"\n",
      "  }\n",
      "} [/INST] {\"name\": \"Douglas Bishop\"}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 14/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Create a benefits property with the value: ['daughter'] based on the schema provided.  \n",
      " {\n",
      "  \"benefits\": {\n",
      "    \"type\": \"array\",\n",
      "    \"items\": {\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"description\": \"List of benefits covered by the insurance\"\n",
      "  }\n",
      "} [/INST] {\"benefits\": [\"daughter\"]}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 15/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Give a sample data for the property schema with the value: Cheyenne Parker.  \n",
      " {\n",
      "  \"full_name\": {\n",
      "    \"type\": \"string\",\n",
      "    \"minLength\": 1,\n",
      "    \"maxLength\": 100,\n",
      "    \"description\": \"Full name of the individual\"\n",
      "  }\n",
      "} [/INST] {\"full_name\": \"Cheyenne Parker\"}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 16/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Create a name property with the value: Kevin Todd based on the schema provided.  \n",
      " {\n",
      "  \"name\": {\n",
      "    \"type\": \"string\",\n",
      "    \"minLength\": 1,\n",
      "    \"maxLength\": 50,\n",
      "    \"description\": \"Full name of the individual\"\n",
      "  }\n",
      "} [/INST] {\"name\": \"Kevin Todd\"}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 17/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Create a sample data for the property schema with the value: Terry Taylor.  \n",
      " {\n",
      "  \"name\": {\n",
      "    \"type\": \"string\",\n",
      "    \"minLength\": 1,\n",
      "    \"maxLength\": 50,\n",
      "    \"description\": \"Full name of the individual\"\n",
      "  }\n",
      "} [/INST] {\"name\": \"Terry Taylor\"}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 18/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "INFO - Generated text: \n",
      " <s><s>[INST] Generate a claim_status property based on the schema.  \n",
      " {\n",
      "  \"claim_status\": {\n",
      "    \"type\": \"object\",\n",
      "    \"properties\": {\n",
      "      \"status\": {\n",
      "        \"type\": \"string\",\n",
      "        \"enum\": [\n",
      "          \"approved\",\n",
      "          \"denied\",\n",
      "          \"pending\"\n",
      "        ],\n",
      "        \"description\": \"Status of the claim\"\n",
      "      },\n",
      "      \"claimstatus_code\": {\n",
      "        \"type\": \"string\",\n",
      "        \"minLength\": 1,\n",
      "        \"maxLength\": 50,\n",
      "        \"description\": \"Code representing the claim status\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "} [/INST] {\"claim_status\": {\"status\": \"pending\", \"claimstatus_code\": \"pending-claim\"}}</s>\n",
      "DEBUG - Starting score: 10\n",
      "DEBUG - Final score: 10\n",
      "INFO - {\"valid_json\": 100.0, \"valid_json_schema\": 100.0, \"domain_field_match\": 100.0}\n",
      "INFO - ------------------------------------------------------------\n",
      "INFO - Processing row 19/112\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m logger.setLevel(logging.DEBUG) \n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m result = json.loads(result)\n\u001b[32m      6\u001b[39m epoch = re.search(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+)$\u001b[39m\u001b[33m'\u001b[39m, adapter_dir).group(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mtest_model\u001b[39m\u001b[34m(model, tokenizer, test_dataset)\u001b[39m\n\u001b[32m    137\u001b[39m inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop_word_ids\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m generated_text = tokenizer.decode(output[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    151\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated text: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\peft\\peft_model.py:886\u001b[39m, in \u001b[36mPeftModel.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    884\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    885\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\synthetic-data-generator\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\generation\\utils.py:2634\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2626\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2627\u001b[39m         input_ids=input_ids,\n\u001b[32m   2628\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2629\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2630\u001b[39m         **model_kwargs,\n\u001b[32m   2631\u001b[39m     )\n\u001b[32m   2633\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2634\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2645\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2646\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2647\u001b[39m         input_ids=input_ids,\n\u001b[32m   2648\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2649\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2650\u001b[39m         **model_kwargs,\n\u001b[32m   2651\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\generation\\utils.py:3618\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3616\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3617\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3618\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3620\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3621\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3622\u001b[39m     outputs,\n\u001b[32m   3623\u001b[39m     model_kwargs,\n\u001b[32m   3624\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3625\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\synthetic-data-generator\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\synthetic-data-generator\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\generic.py:959\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    958\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m959\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    961\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:434\u001b[39m, in \u001b[36mMistralForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    415\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    416\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    417\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    419\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    432\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\synthetic-data-generator\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\synthetic-data-generator\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\generic.py:1083\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1080\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1081\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1085\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:364\u001b[39m, in \u001b[36mMistralModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    376\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    377\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\synthetic-data-generator\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\synthetic-data-generator\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:228\u001b[39m, in \u001b[36mMistralDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    226\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\synthetic-data-generator\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\synthetic-data-generator\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:151\u001b[39m, in \u001b[36mMistralAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m input_shape = hidden_states.shape[:-\u001b[32m1\u001b[39m]\n\u001b[32m    149\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m query_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    152\u001b[39m key_states = \u001b[38;5;28mself\u001b[39m.k_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    153\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\synthetic-data-generator\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\synthetic-data-generator\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\peft\\tuners\\lora\\bnb.py:504\u001b[39m, in \u001b[36mLinear4bit.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m     output = lora_B(lora_A(dropout(x))) * scaling\n\u001b[32m    503\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m requires_conversion:\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m         output = \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m     result = result + output\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.DEBUG) \n",
    "result = test_model(model, tokenizer, split_datasets[\"test\"])\n",
    "\n",
    "result = json.loads(result)\n",
    "\n",
    "epoch = re.search(r'(\\d+)$', adapter_dir).group(1)\n",
    "print(f\"Logging results for epoch {epoch} to MLflow...\")\n",
    "mlflow.log_metric(\"syntactic-accuracy\", result['valid_json'], step=int(epoch))\n",
    "mlflow.log_metric(\"schema-accuracy\", result['valid_json_schema'], step=int(epoch))\n",
    "mlflow.log_metric(\"domain-field-match\", result['domain_field_match'], step=int(epoch))\n",
    "\n",
    "mlflow.end_run()\n",
    "logger.info(f\"Final Result: {result}\")\n",
    "logger.info(\"Evaluation completed and results logged to MLflow.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthetic-data-generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
