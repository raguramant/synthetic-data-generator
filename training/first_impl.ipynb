{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08fc88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\admin\\miniconda3\\lib\\site-packages (0.46.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\admin\\miniconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from bitsandbytes) (2.7.0+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from bitsandbytes) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\miniconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\admin\\miniconda3\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from accelerate) (0.34.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\miniconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\miniconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\miniconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\users\\admin\\miniconda3\\lib\\site-packages (4.54.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\miniconda3\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from transformers) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\miniconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bitsandbytes accelerate\n",
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855e895",
   "metadata": {},
   "source": [
    "### Login to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc7b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "\n",
    "#notebook_login()\n",
    "# Set the Hugging Face token as an environment variable\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d4dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d5c8b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 27 14:09:45 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.40                 Driver Version: 576.40         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5070 Ti   WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   43C    P8             18W /  300W |       0MiB /  16303MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebd775",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "#Model configs\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "checkpoint_dir = \"../output/checkpoints\"\n",
    "model_output_dir = \"../output/final_adapter\"\n",
    "training_data = \"../data/combinations.jsonl\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca8082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cb4c34e3764636aceb841da86d7939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config ,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7458813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(model.parameters()).dtype\n",
    "\n",
    "# for name, module in model.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf4511",
   "metadata": {},
   "source": [
    "### Configure PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,631,488 || all params: 7,261,655,040 || trainable%: 0.1877\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6daa7266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7fa15c",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b21ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bb062d0b924f209c1cd6673d1778ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20ee28958d3471da051319857b510c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ac375bbea84e9b9ea5a2aba73a8fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 70\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "# load the dataset and split it into train, validation and test sets\n",
    "dataset = load_dataset(\"json\", data_files=training_data, split='train')\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "train_temp_split = shuffled_dataset.train_test_split(test_size=0.3) #30% for validation and test\n",
    "temp_dataset = train_temp_split['test']\n",
    "validation_test_split = temp_dataset.train_test_split(test_size=1/3)# 10% for validation and 20% for test\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_temp_split['train'],\n",
    "    'validation': validation_test_split['train'],\n",
    "    'test': validation_test_split['test']\n",
    "})\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "MAX_LENGTH = 1500\n",
    "\n",
    "\n",
    "def tokenize_with_loss_mask(example):\n",
    "    chat_str = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)\n",
    "    tokenized = tokenizer(chat_str, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    labels = []\n",
    "\n",
    "    seq_start_idx = 0 # start of sequence\n",
    "    total_length = len(chat_str)\n",
    "\n",
    "    while seq_start_idx < total_length:\n",
    "\n",
    "        # print(\"-----------------------\")\n",
    "        seq_end_idx = chat_str.find(\"</s>\", seq_start_idx)+ len(\"</s>\") # end of sequence\n",
    "        if seq_end_idx == -1:\n",
    "            break\n",
    "\n",
    "        sequence = chat_str[seq_start_idx:seq_end_idx]\n",
    "        end_user_idx = sequence.find(\"[/INST]\")+ len(\"[/INST]\") # end of user message\n",
    "        end_assistant_idx = sequence.find(\"</s>\")\n",
    "        user_content = sequence[:end_user_idx]\n",
    "        assistant_content = sequence[end_user_idx:end_assistant_idx]\n",
    "\n",
    "        seq_tokens = tokenizer(sequence, add_special_tokens=False)[\"input_ids\"]\n",
    "        user_tokens = tokenizer(user_content, add_special_tokens=False)[\"input_ids\"]\n",
    "        assistant_tokens = tokenizer(assistant_content, add_special_tokens=False)[\"input_ids\"]\n",
    "        \n",
    "        labels.extend([-100] * len(user_tokens))\n",
    "        labels.extend(assistant_tokens)\n",
    "        labels.extend([-100] * (len(seq_tokens) - len(user_tokens) - len(assistant_tokens)))\n",
    "        \n",
    "        seq_start_idx = seq_end_idx\n",
    "\n",
    "    labels = [-100] * (len(input_ids) - len(labels)) + labels  # Pad to max length\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    if len(tokenized[\"input_ids\"]) > MAX_LENGTH:\n",
    "        print(\"Warning: Input sequence exceeds max length for inputs, truncating.\")\n",
    "    if len(tokenized[\"labels\"]) > MAX_LENGTH:\n",
    "        print(\"Warning: Input sequence exceeds max length for labels, truncating.\")\n",
    "    if len(tokenized[\"input_ids\"]) != len(tokenized[\"labels\"]):\n",
    "        print(\"Error: Input and label lengths do not match after processing.\")\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = split_datasets.map(tokenize_with_loss_mask, remove_columns=split_datasets[\"train\"].column_names)\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0656176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset, Dataset\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# raw_dataset = load_dataset(\"json\", data_files=\"../data/combinations.jsonl\")[\"train\"]\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# def tokenize(example):\n",
    "#     model_inputs = tokenizer(\n",
    "#         tokenizer.apply_chat_template(example[\"messages\"], tokenize=False),\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=1024\n",
    "#     )\n",
    "#     model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "#     return model_inputs\n",
    "\n",
    "# tokenized_dataset = raw_dataset.map(tokenize)\n",
    "\n",
    "# tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8eecef",
   "metadata": {},
   "source": [
    "### Tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266488a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from checkpoint: ../output/checkpoints\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 03:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.484215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.765200</td>\n",
       "      <td>0.478396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('../output/final_adapter\\\\tokenizer_config.json',\n",
       " '../output/final_adapter\\\\special_tokens_map.json',\n",
       " '../output/final_adapter\\\\chat_template.jinja',\n",
       " '../output/final_adapter\\\\tokenizer.model',\n",
       " '../output/final_adapter\\\\added_tokens.json',\n",
       " '../output/final_adapter\\\\tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1, # 1 sample per device due to GPU memory constraints\n",
    "    per_device_eval_batch_size=1, # 1 sample per device due to GPU memory constraints\n",
    "    gradient_accumulation_steps=8, # to accumulate gradients update over multiple steps to simulate larger batch sizes\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,#10\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # save_steps=500,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    load_best_model_at_end=True,\n",
    "    output_dir=checkpoint_dir,\n",
    "    label_names=[\"labels\"],\n",
    "    disable_tqdm=False, # enable tqdm progress bars\n",
    "    gradient_checkpointing=True, # to train large models with limited GPU memory\n",
    "    save_total_limit=4, # keep only the last 4 checkpoints to save space\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "resume_from_checkpoint = None\n",
    "if os.path.exists(checkpoint_dir) and len(os.listdir(checkpoint_dir)) > 0:\n",
    "    resume_from_checkpoint = checkpoint_dir\n",
    "    print(f\"Resuming training from checkpoint: {resume_from_checkpoint}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(model_output_dir)\n",
    "tokenizer.save_pretrained(model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cfdad",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a46c7e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4726983904838562, 'eval_runtime': 4.9767, 'eval_samples_per_second': 2.009, 'eval_steps_per_second': 2.009, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(eval_dataset=tokenized_dataset['test'])\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645a6d5",
   "metadata": {},
   "source": [
    "### Reload model and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96726d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac9f55347f04c57a32657c55d905a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from peft import PeftModel\n",
    "\n",
    "# # Load base model\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#                                                   quantization_config=quantization_config , \n",
    "#                                                   device_map=\"auto\")\n",
    "\n",
    "# # Load QLoRA adapter on top\n",
    "# model = PeftModel.from_pretrained(base_model, model_output_dir)\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c58b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32768\n",
      "Output:  <INST>You are a Json test data generator generating valid json data based on a json schema. Given the below schema,\n",
      "\n",
      "    ### Schema: {'$schema': 'http://json-schema.org/draft-07/schema#', 'title': 'X12 NM1 Segment', 'description': \"Schema for X12 NM1 segment, which contains subscriber's or organization name information.\", 'type': 'object', 'properties': {'entity_identifier_code': {'type': 'string', 'description': 'Entity Identifier Code (e.g., IL for Insured or 03 for dependent or PR for Payer or 1P for Provider)', 'enum': ['IL', '03', 'PR', '1P']}, 'entity_type_qualifier': {'type': 'string', 'description': 'Entity Type Qualifier (1 = Person, 2 = Non-Person Entity)', 'enum': ['1', '2']}, 'name_last_or_organizationName': {'type': 'string', 'description': 'Name Last or Organization Name', 'minLength': 1, 'maxLength': 60}, 'name_first': {'type': 'string', 'description': 'Name First', 'nullable': True, 'minLength': 1, 'maxLength': 35}, 'name_middle': {'type': 'string', 'description': 'Name Middle', 'nullable': True, 'minLength': 1, 'maxLength': 25}, 'name_prefix': {'type': 'string', 'description': 'Name Prefix', 'nullable': True, 'minLength': 1, 'maxLength': 10}, 'name_suffix': {'type': 'string', 'description': 'Name Suffix', 'nullable': True, 'minLength': 1, 'maxLength': 10}, 'identification_code_qualifier': {'type': 'string', 'description': 'Identification Code Qualifier (e.g., MI = Member ID, XX = National Provider Identifier, SV = Service Provider Number, FI = Federal Tax ID, PI = Payer Identifier, ZZ = Mutually Defined)', 'enum': ['MI', 'XX', 'SV', 'FI', 'PI', 'ZZ'], 'nullable': True}, 'identification_code': {'type': 'string', 'description': 'Identification Code', 'nullable': True, 'minLength': 2, 'maxLength': 80}, 'nm1_segment': {'type': 'string', 'description': 'NM1 Segment format of the data (e.g., NM1*IL*1*DOE*JOHN****MI*123456789~)', 'nullable': True, 'maxLength': 255}}, 'required': ['entity_identifier_code', 'entity_type_qualifier', 'name_last_or_organizationName', 'nm1_segment']}\n",
      "\n",
      " Generate a sample json that confirms to the given schema</INST>\n",
      "{'entity_identifier_code': 'IL', 'entity_type_qualifier': '2', 'name_last_or_organizationName': 'Pope', 'name_first': 'Hannah', 'name_middle': 'W', 'name_prefix': 'Dr.', 'name_suffix': 'MD', 'identification_code_qualifier': 'XX', 'identification_code': 'VqXeTzPz', 'nm1_segment': 'NM1*IL*2*Pope*Hannah*W*Dr.*MD*XX*VqXeTzPz~'}\n"
     ]
    }
   ],
   "source": [
    "# print(config.max_position_embeddings)\n",
    "\n",
    "# prompt = \" <s><INST>You are a Json test data generator generating valid json data based on a json schema. Given the below schema,\\n\\n    ### Schema: {'$schema': 'http://json-schema.org/draft-07/schema#', 'title': 'X12 NM1 Segment', 'description': \\\"Schema for X12 NM1 segment, which contains subscriber's or organization name information.\\\", 'type': 'object', 'properties': {'entity_identifier_code': {'type': 'string', 'description': 'Entity Identifier Code (e.g., IL for Insured or 03 for dependent or PR for Payer or 1P for Provider)', 'enum': ['IL', '03', 'PR', '1P']}, 'entity_type_qualifier': {'type': 'string', 'description': 'Entity Type Qualifier (1 = Person, 2 = Non-Person Entity)', 'enum': ['1', '2']}, 'name_last_or_organizationName': {'type': 'string', 'description': 'Name Last or Organization Name', 'minLength': 1, 'maxLength': 60}, 'name_first': {'type': 'string', 'description': 'Name First', 'nullable': True, 'minLength': 1, 'maxLength': 35}, 'name_middle': {'type': 'string', 'description': 'Name Middle', 'nullable': True, 'minLength': 1, 'maxLength': 25}, 'name_prefix': {'type': 'string', 'description': 'Name Prefix', 'nullable': True, 'minLength': 1, 'maxLength': 10}, 'name_suffix': {'type': 'string', 'description': 'Name Suffix', 'nullable': True, 'minLength': 1, 'maxLength': 10}, 'identification_code_qualifier': {'type': 'string', 'description': 'Identification Code Qualifier (e.g., MI = Member ID, XX = National Provider Identifier, SV = Service Provider Number, FI = Federal Tax ID, PI = Payer Identifier, ZZ = Mutually Defined)', 'enum': ['MI', 'XX', 'SV', 'FI', 'PI', 'ZZ'], 'nullable': True}, 'identification_code': {'type': 'string', 'description': 'Identification Code', 'nullable': True, 'minLength': 2, 'maxLength': 80}, 'nm1_segment': {'type': 'string', 'description': 'NM1 Segment format of the data (e.g., NM1*IL*1*DOE*JOHN****MI*123456789~)', 'nullable': True, 'maxLength': 255}}, 'required': ['entity_identifier_code', 'entity_type_qualifier', 'name_last_or_organizationName', 'nm1_segment']}\\n\\n Generate a sample json that confirms to the given schema</INST>\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model.generate(\n",
    "#         **inputs,\n",
    "#         max_new_tokens=1000,\n",
    "#         temperature=0.7,\n",
    "#         do_sample=True,\n",
    "#         top_p=0.9,\n",
    "#         eos_token_id=tokenizer.eos_token_id\n",
    "#     )\n",
    "    \n",
    "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(\"Output: \", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e531c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 27 14:10:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.40                 Driver Version: 576.40         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5070 Ti   WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   43C    P8             18W /  300W |       0MiB /  16303MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10336065",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "150f5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "# function to extact JSON from a string\n",
    "def extract_json_from_string(input_string):\n",
    "    try:\n",
    "        # Use a regular expression to find the first JSON object in the string\n",
    "        json_match = re.search(r'\\{.*?\\}', input_string, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(0)  # Extract the JSON string\n",
    "            return json.loads(json_str)  # Parse and return the JSON object\n",
    "        else:\n",
    "            print(\"No JSON object found in the string.\")\n",
    "            return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Validate a JSON string against a schema\n",
    "def is_valid_json_schema(json_generated, schema):\n",
    "    try:\n",
    "        validate(instance=json_generated, schema=schema)\n",
    "        return True\n",
    "    except (json.JSONDecodeError, ValidationError):\n",
    "        return False\n",
    "    \n",
    "# Compare two JSON strings at field level and match field values\n",
    "def compare_json_field_values(json_ref, json_gen):\n",
    "    obj1=json_ref\n",
    "    obj2=json_gen\n",
    "    matching_fields = []\n",
    "    matching_values = 0\n",
    "    total_fields = max(len(obj1), len(obj2))\n",
    "    for key in obj1:\n",
    "        if key in obj2:\n",
    "            matching_fields.append(key)\n",
    "            if obj1[key] == obj2[key]:\n",
    "                matching_values += 1\n",
    "    score = matching_values\n",
    "    percentage = score / total_fields if total_fields > 0 else 0\n",
    "    result = {\n",
    "        \"matching_fields\": matching_fields,\n",
    "        \"matching_values\": score,\n",
    "        \"total_fields\": total_fields,\n",
    "        \"percentage\": percentage\n",
    "    }\n",
    "    return result['percentage']\n",
    "\n",
    "\n",
    "def test_model(adapter_dir, test_dataset):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    valid_json = []\n",
    "    valid_json_schema = []\n",
    "    comparison_results = []\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                    quantization_config=quantization_config , \n",
    "                                                    device_map=\"auto\")\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, model_output_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n",
    "\n",
    "    for i, row in enumerate(test_dataset):\n",
    "\n",
    "        prompt = f\"<s><INST>{row['messages'][0]['content']}</INST>\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1000,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        if(\"</INST>\" in generated_text):\n",
    "            generated_json = extract_json_from_string(generated_text.split(\"<INST>\")[-1].strip())\n",
    "            gold_json = extract_json_from_string(row['messages'][1]['content'])\n",
    "            schema = extract_json_from_string(generated_text.split(\"<INST>\")[-2].strip())\n",
    "        \n",
    "        if generated_json is not None:\n",
    "            valid_json.append(True)\n",
    "        else:\n",
    "            valid_json.append(False)\n",
    "\n",
    "        \n",
    "        if not is_valid_json_schema(generated_json, schema):\n",
    "            valid_json_schema.append(False)\n",
    "        else:\n",
    "            valid_json_schema.append(True)\n",
    "        \n",
    "        comparison_score = compare_json_field_values(gold_json, generated_json)\n",
    "        comparison_results.append(comparison_score)\n",
    "\n",
    "    \n",
    "    result = {\n",
    "        \"valid_json\": np.mean(valid_json) * 100,\n",
    "        \"valid_json_schema\": np.mean(valid_json_schema) * 100,\n",
    "        \"comparison_results\": np.mean(comparison_results) * 100\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeefcfcf",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ffc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14912958cc994edc8e0e471f32e9512e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'PeftModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mtest_model\u001b[39m\u001b[34m(adapter_dir, test_dataset)\u001b[39m\n\u001b[32m     59\u001b[39m comparison_results = []\n\u001b[32m     61\u001b[39m base_model = AutoModelForCausalLM.from_pretrained(model_name, \n\u001b[32m     62\u001b[39m                                                 quantization_config=quantization_config , \n\u001b[32m     63\u001b[39m                                                 device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m model = \u001b[43mPeftModel\u001b[49m.from_pretrained(base_model, model_output_dir)\n\u001b[32m     66\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataset):\n",
      "\u001b[31mNameError\u001b[39m: name 'PeftModel' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
